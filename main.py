#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SafeDocs - AI –∑–∞—â–∏—Ç–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–í–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ
"""

import os
import io
import json
import uuid
import logging
from datetime import datetime
from typing import List, Dict, Optional, Union
from pathlib import Path

# Web Framework
from fastapi import FastAPI, File, UploadFile, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from fastapi.encoders import jsonable_encoder

# Data Models
from pydantic import BaseModel, Field, validator

# File Processing
import PyPDF2
from PIL import Image, ImageEnhance
import pytesseract

# Environment
from dotenv import load_dotenv

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# ============================================================================
# –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–•
# ============================================================================

class HighlightData(BaseModel):
    """–î–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    start: int = Field(..., description="–ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è")
    end: int = Field(..., description="–ö–æ–Ω–µ—á–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è")
    type: str = Field(..., description="–¢–∏–ø: risk, benefit, unclear")
    text: str = Field(..., description="–ü–æ–¥—Å–≤–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç")
    category: str = Field(..., description="–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–∞—Ö–æ–¥–∫–∏")
    severity: Optional[str] = Field(None, description="–£—Ä–æ–≤–µ–Ω—å —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏")

class RiskData(BaseModel):
    """–î–∞–Ω–Ω—ã–µ –æ —Ä–∏—Å–∫–µ"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: str = Field(..., description="–¢–∏–ø —Ä–∏—Å–∫–∞")
    keyword: str = Field(..., description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
    context: str = Field(..., description="–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è")
    position: int = Field(..., description="–ü–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–µ")
    severity: str = Field(..., description="–£—Ä–æ–≤–µ–Ω—å: –Ω–∏–∑–∫–∏–π, —Å—Ä–µ–¥–Ω–∏–π, –≤—ã—Å–æ–∫–∏–π")
    recommendation: str = Field(..., description="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è")
    legal_reference: Optional[str] = Field(None, description="–°—Å—ã–ª–∫–∞ –Ω–∞ –∑–∞–∫–æ–Ω")

class BenefitData(BaseModel):
    """–î–∞–Ω–Ω—ã–µ –æ –≤—ã–≥–æ–¥–µ"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    type: str = Field(..., description="–¢–∏–ø –≤—ã–≥–æ–¥—ã")
    keyword: str = Field(..., description="–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ")
    context: str = Field(..., description="–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è")
    position: int = Field(..., description="–ü–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–µ")
    value: str = Field(..., description="–¶–µ–Ω–Ω–æ—Å—Ç—å: –Ω–∏–∑–∫–∞—è, —Å—Ä–µ–¥–Ω—è—è, –≤—ã—Å–æ–∫–∞—è")
    description: str = Field(..., description="–û–ø–∏—Å–∞–Ω–∏–µ –≤—ã–≥–æ–¥—ã")

class UnclearTermData(BaseModel):
    """–î–∞–Ω–Ω—ã–µ –æ –Ω–µ—è—Å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–µ"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    phrase: str = Field(..., description="–ù–µ—è—Å–Ω–∞—è —Ñ—Ä–∞–∑–∞")
    context: str = Field(..., description="–ö–æ–Ω—Ç–µ–∫—Å—Ç")
    position: int = Field(..., description="–ü–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–µ")
    explanation: str = Field(..., description="–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã")
    suggestion: str = Field(..., description="–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é")
    legal_clarification: Optional[str] = Field(None, description="–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏–µ")

class DocumentAnalysis(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    document_id: str = Field(..., description="–£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    filename: str = Field(..., description="–ò–º—è —Ñ–∞–π–ª–∞")
    file_type: str = Field(..., description="–¢–∏–ø —Ñ–∞–π–ª–∞")
    file_size: int = Field(..., description="–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö")
    
    # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    text_content: str = Field(..., description="–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ")
    full_text: str = Field(..., description="–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    word_count: int = Field(..., description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    risks: List[RiskData] = Field(default_factory=list, description="–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏")
    benefits: List[BenefitData] = Field(default_factory=list, description="–í—ã–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è")
    unclear_terms: List[UnclearTermData] = Field(default_factory=list, description="–ù–µ—è—Å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã")
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
    overall_rating: str = Field(..., description="–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
    risk_score: float = Field(..., description="–ß–∏—Å–ª–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞ (0-100)")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    summary: str = Field(..., description="–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ")
    recommendations: List[str] = Field(default_factory=list, description="–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    highlights: List[HighlightData] = Field(default_factory=list, description="–ü–æ–¥—Å–≤–µ—Ç–∫–∞")
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    processed_at: str = Field(..., description="–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    processing_time: float = Field(..., description="–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö")
    version: str = Field(default="1.0.0", description="–í–µ—Ä—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")

class ChatMessage(BaseModel):
    """–°–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç–µ"""
    document_id: str = Field(..., description="ID –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    question: str = Field(..., max_length=1000, description="–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")

class ChatResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —á–∞—Ç–∞"""
    answer: str = Field(..., description="–û—Ç–≤–µ—Ç AI")
    confidence: float = Field(..., description="–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ")
    relevant_sections: List[str] = Field(default_factory=list, description="–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏")
    document_id: str = Field(..., description="ID –¥–æ–∫—É–º–µ–Ω—Ç–∞")

# ============================================================================
# –û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–û–í
# ============================================================================

class FileProcessor:
    """–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤"""
    
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
    SUPPORTED_TYPES = {
        'application/pdf': 'pdf',
        'image/jpeg': 'image',
        'image/jpg': 'image',
        'image/png': 'image',
        'text/plain': 'text',
        'application/msword': 'doc',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx'
    }
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (20MB)
    MAX_FILE_SIZE = 20 * 1024 * 1024
    
    @staticmethod
    def validate_file(file: UploadFile) -> Dict[str, any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        if file.content_type not in FileProcessor.SUPPORTED_TYPES:
            raise HTTPException(
                status_code=400,
                detail=f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file.content_type}. "
                       f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: {', '.join(FileProcessor.SUPPORTED_TYPES.keys())}"
            )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if not file.filename:
            raise HTTPException(status_code=400, detail="–ò–º—è —Ñ–∞–π–ª–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–æ")
        
        return {
            'file_type': FileProcessor.SUPPORTED_TYPES[file.content_type],
            'content_type': file.content_type,
            'filename': file.filename
        }
    
    @staticmethod
    async def process_file(file: UploadFile) -> Dict[str, any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        file_info = FileProcessor.validate_file(file)
        
        # –ß—Ç–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        try:
            file_content = await file.read()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            raise HTTPException(status_code=400, detail="–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        if len(file_content) > FileProcessor.MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail=f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {len(file_content)} –±–∞–π—Ç. "
                       f"–ú–∞–∫—Å–∏–º—É–º: {FileProcessor.MAX_FILE_SIZE} –±–∞–π—Ç"
            )
        
        if len(file_content) == 0:
            raise HTTPException(status_code=400, detail="–§–∞–π–ª –ø—É—Å—Ç–æ–π")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        file_type = file_info['file_type']
        
        try:
            if file_type == 'pdf':
                text = FileProcessor._extract_from_pdf(file_content)
            elif file_type == 'image':
                text = FileProcessor._extract_from_image(file_content)
            elif file_type == 'text':
                text = FileProcessor._extract_from_text(file_content)
            else:
                raise HTTPException(status_code=400, detail=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {file_type} –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if not text or len(text.strip()) < 50:
                raise HTTPException(
                    status_code=400,
                    detail="–ò–∑–≤–ª–µ—á–µ–Ω–æ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–º–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤)"
                )
            
            return {
                'text': text.strip(),
                'file_type': file_type,
                'filename': file_info['filename'],
                'file_size': len(file_content),
                'word_count': len(text.split())
            }
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {file_type}: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {str(e)}")
    
    @staticmethod
    def _extract_from_pdf(content: bytes) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
        try:
            pdf_file = io.BytesIO(content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            if len(pdf_reader.pages) == 0:
                raise Exception("PDF –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            text_parts = []
            
            for page_num, page in enumerate(pdf_reader.pages, 1):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_parts.append(f"\n=== –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ===\n")
                        text_parts.append(page_text)
                        text_parts.append("\n")
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                    continue
            
            full_text = "".join(text_parts)
            
            if not full_text.strip():
                raise Exception("PDF –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–∑–≤–ª–µ–∫–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
            
            return full_text.strip()
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF: {str(e)}")
    
    @staticmethod
    def _extract_from_image(content: bytes) -> str:
        """–í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ OCR –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            # –û—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = Image.open(io.BytesIO(content))
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode not in ('RGB', 'L'):
                image = image.convert('RGB')
            
            # –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image = FileProcessor._enhance_image_for_ocr(image)
            
            # OCR —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            custom_config = r'--oem 3 --psm 6 -l rus+eng'
            
            try:
                text = pytesseract.image_to_string(image, config=custom_config)
            except Exception as ocr_error:
                logger.warning(f"OCR —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –Ω–µ —É–¥–∞–ª—Å—è: {ocr_error}")
                # Fallback - –ø—Ä–æ—Å—Ç–æ–π OCR
                text = pytesseract.image_to_string(image, lang='rus+eng')
            
            if not text.strip():
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏")
            
            return text.strip()
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ OCR: {str(e)}")
    
    @staticmethod
    def _enhance_image_for_ocr(image: Image.Image) -> Image.Image:
        """–£–ª—É—á—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ OCR"""
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        width, height = image.size
        if width < 1500:
            scale_factor = 1500 / width
            new_width = int(width * scale_factor)
            new_height = int(height * scale_factor)
            image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # –ü–æ–≤—ã—à–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.2)
        
        # –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ä–µ–∑–∫–æ—Å—Ç–∏
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.1)
        
        return image
    
    @staticmethod
    def _extract_from_text(content: bytes) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–∫–∞—Ö
            encodings = ['utf-8', 'utf-16', 'cp1251', 'latin1']
            
            for encoding in encodings:
                try:
                    text = content.decode(encoding)
                    return text.strip()
                except UnicodeDecodeError:
                    continue
            
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª")
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞: {str(e)}")

# ============================================================================
# AI –ê–ù–ê–õ–ò–ó–ê–¢–û–† –î–û–ö–£–ú–ï–ù–¢–û–í
# ============================================================================

class DocumentAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–∏—Å–∫–æ–≤
    RISK_PATTERNS = {
        "—à—Ç—Ä–∞—Ñ–Ω—ã–µ_—Å–∞–Ω–∫—Ü–∏–∏": {
            "keywords": ["—à—Ç—Ä–∞—Ñ", "–ø–µ–Ω—è", "–Ω–µ—É—Å—Ç–æ–π–∫–∞", "—Å–∞–Ω–∫—Ü–∏–∏", "–ø–µ–Ω–∏", "–≤–∑—ã—Å–∫–∞–Ω–∏–µ"],
            "severity": "–≤—ã—Å–æ–∫–∏–π",
            "description": "–®—Ç—Ä–∞—Ñ–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–∞–∫–∞–∑–∞–Ω–∏—è"
        },
        "–æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ_—É—Å–ª–æ–≤–∏—è": {
            "keywords": ["–æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω", "–≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è", "–ø–æ —Å–≤–æ–µ–º—É —É—Å–º–æ—Ç—Ä–µ–Ω–∏—é", "–≤–ø—Ä–∞–≤–µ —Ä–∞—Å—Ç–æ—Ä–≥–Ω—É—Ç—å", "–±–µ–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è"],
            "severity": "–≤—ã—Å–æ–∫–∏–π", 
            "description": "–û–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —É—Å–ª–æ–≤–∏—è —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è"
        },
        "–≤—ã—Å–æ–∫–∞—è_–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": {
            "keywords": ["–ø–æ–ª–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ –≤—Å–µ—Ö", "—Å–æ–ª–∏–¥–∞—Ä–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"],
            "severity": "–≤—ã—Å–æ–∫–∏–π",
            "description": "–ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"
        },
        "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ_—Å—Ä–æ–∫–∏": {
            "keywords": ["—Ä–∞–∑—É–º–Ω—ã–π —Å—Ä–æ–∫", "–≤ –∫—Ä–∞—Ç—á–∞–π—à–∏–µ —Å—Ä–æ–∫–∏", "–Ω–µ–∑–∞–º–µ–¥–ª–∏—Ç–µ–ª—å–Ω–æ", "–≤ —Å—Ä–æ—á–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ", "–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ"],
            "severity": "—Å—Ä–µ–¥–Ω–∏–π",
            "description": "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏"
        },
        "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ_—Ä–∏—Å–∫–∏": {
            "keywords": ["–∑–∞ —Å–≤–æ–π —Å—á–µ—Ç", "–±–µ–∑ –≤–æ–∑–º–µ—â–µ–Ω–∏—è", "–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ", "—É–±—ã—Ç–∫–∏ –ø–æ–∫—É–ø–∞—Ç–µ–ª—è", "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞"],
            "severity": "—Å—Ä–µ–¥–Ω–∏–π",
            "description": "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞"
        },
        "—Ñ–æ—Ä—Å_–º–∞–∂–æ—Ä": {
            "keywords": ["—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä", "–Ω–µ–ø—Ä–µ–æ–¥–æ–ª–∏–º–∞—è —Å–∏–ª–∞", "—á—Ä–µ–∑–≤—ã—á–∞–π–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞"],
            "severity": "—Å—Ä–µ–¥–Ω–∏–π",
            "description": "–£—Å–ª–æ–≤–∏—è —Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–∞ –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏"
        }
    }
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã–≥–æ–¥
    BENEFIT_PATTERNS = {
        "–≥–∞—Ä–∞–Ω—Ç–∏–∏": {
            "keywords": ["–≥–∞—Ä–∞–Ω—Ç–∏—è", "–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç", "–æ–±—è–∑—É–µ—Ç—Å—è", "–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ"],
            "value": "–≤—ã—Å–æ–∫–∞—è",
            "description": "–ì–∞—Ä–∞–Ω—Ç–∏–π–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞"
        },
        "–∑–∞—â–∏—Ç–∞_–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤": {
            "keywords": ["—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ —É—â–µ—Ä–±–∞", "–∑–∞—â–∏—Ç–∞ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤"],
            "value": "–≤—ã—Å–æ–∫–∞—è",
            "description": "–ó–∞—â–∏—Ç–∞ –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –∏ –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏"
        },
        "–≥–∏–±–∫–∏–µ_—É—Å–ª–æ–≤–∏—è": {
            "keywords": ["–ø–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—é", "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è", "–ø—Ä–∞–≤–æ –≤—ã–±–æ—Ä–∞", "–ø–æ –¥–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"],
            "value": "—Å—Ä–µ–¥–Ω—è—è",
            "description": "–ì–∏–±–∫–∏–µ —É—Å–ª–æ–≤–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞"
        },
        "–≤–æ–∑–≤—Ä–∞—Ç_—Å—Ä–µ–¥—Å—Ç–≤": {
            "keywords": ["–≤–æ–∑–≤—Ä–∞—Ç", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–∞", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è —Ä–∞—Å—Ö–æ–¥–æ–≤", "–≤–æ–∑–≤—Ä–∞—Ç –¥–µ–Ω–µ–≥"],
            "value": "–≤—ã—Å–æ–∫–∞—è",
            "description": "–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–æ–∑–≤—Ä–∞—Ç–∞ —Å—Ä–µ–¥—Å—Ç–≤"
        },
        "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ_—É—Å–ª—É–≥–∏": {
            "keywords": ["–±–µ—Å–ø–ª–∞—Ç–Ω–æ", "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ", "–≤ –ø–æ–¥–∞—Ä–æ–∫", "–±–µ–∑ –¥–æ–ø–ª–∞—Ç—ã"],
            "value": "—Å—Ä–µ–¥–Ω—è—è",
            "description": "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —É—Å–ª—É–≥–∏"
        }
    }
    
    # –ù–µ—è—Å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    UNCLEAR_PATTERNS = [
        "–≤ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ",
        "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ä—ã", 
        "–Ω–∞–¥–ª–µ–∂–∞—â–∏–º –æ–±—Ä–∞–∑–æ–º",
        "—Ä–∞–∑—É–º–Ω—ã–π —Å—Ä–æ–∫",
        "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è",
        "—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞",
        "–∏–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞",
        "–ø–æ –æ–±–æ—é–¥–Ω–æ–º—É —Å–æ–≥–ª–∞—Å–∏—é",
        "–≤ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö",
        "–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å–ª—É—á–∞—è—Ö",
        "–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏",
        "–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤"
    ]
    
    @staticmethod
    def analyze(text: str, filename: str, file_type: str, file_size: int, word_count: int) -> DocumentAnalysis:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        start_time = datetime.now()
        
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            text_lower = text.lower()
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤
            risks = DocumentAnalyzer._analyze_risks(text, text_lower)
            
            # –ê–Ω–∞–ª–∏–∑ –≤—ã–≥–æ–¥
            benefits = DocumentAnalyzer._analyze_benefits(text, text_lower)
            
            # –ê–Ω–∞–ª–∏–∑ –Ω–µ—è—Å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            unclear_terms = DocumentAnalyzer._analyze_unclear_terms(text, text_lower)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–¥—Å–≤–µ—Ç–∫–∏
            highlights = DocumentAnalyzer._create_highlights(risks, benefits, unclear_terms)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏
            overall_rating, risk_score = DocumentAnalyzer._calculate_rating(risks, benefits, unclear_terms)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            recommendations = DocumentAnalyzer._generate_recommendations(risks, benefits, unclear_terms)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ
            summary = DocumentAnalyzer._create_summary(filename, risks, benefits, unclear_terms, overall_rating)
            
            # –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return DocumentAnalysis(
                document_id=str(uuid.uuid4()),
                filename=filename,
                file_type=file_type,
                file_size=file_size,
                text_content=text[:2000] + "..." if len(text) > 2000 else text,
                full_text=text,
                word_count=word_count,
                risks=risks,
                benefits=benefits,
                unclear_terms=unclear_terms,
                overall_rating=overall_rating,
                risk_score=risk_score,
                summary=summary,
                recommendations=recommendations,
                highlights=highlights,
                processed_at=datetime.now().isoformat(),
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
    
    @staticmethod
    def _analyze_risks(text: str, text_lower: str) -> List[RiskData]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        risks = []
        
        for risk_type, pattern_data in DocumentAnalyzer.RISK_PATTERNS.items():
            for keyword in pattern_data["keywords"]:
                positions = DocumentAnalyzer._find_all_positions(text_lower, keyword)
                
                for pos in positions:
                    context = DocumentAnalyzer._extract_context(text, pos, len(keyword))
                    
                    risk = RiskData(
                        type=risk_type.replace("_", " ").title(),
                        keyword=keyword,
                        context=context,
                        position=pos,
                        severity=pattern_data["severity"],
                        recommendation=DocumentAnalyzer._get_risk_recommendation(risk_type, keyword),
                        legal_reference=DocumentAnalyzer._get_legal_reference(risk_type)
                    )
                    
                    risks.append(risk)
                    # –û–¥–∏–Ω —Ä–∏—Å–∫ –Ω–∞ —Ç–∏–ø –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                    break
                
                if risks and risks[-1].type == risk_type.replace("_", " ").title():
                    break
        
        return risks
    
    @staticmethod
    def _analyze_benefits(text: str, text_lower: str) -> List[BenefitData]:
        """–ê–Ω–∞–ª–∏–∑ –≤—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π"""
        benefits = []
        
        for benefit_type, pattern_data in DocumentAnalyzer.BENEFIT_PATTERNS.items():
            for keyword in pattern_data["keywords"]:
                positions = DocumentAnalyzer._find_all_positions(text_lower, keyword)
                
                for pos in positions:
                    context = DocumentAnalyzer._extract_context(text, pos, len(keyword))
                    
                    benefit = BenefitData(
                        type=benefit_type.replace("_", " ").title(),
                        keyword=keyword,
                        context=context,
                        position=pos,
                        value=pattern_data["value"],
                        description=pattern_data["description"]
                    )
                    
                    benefits.append(benefit)
                    break
                
                if benefits and benefits[-1].type == benefit_type.replace("_", " ").title():
                    break
        
        return benefits
    
    @staticmethod
    def _analyze_unclear_terms(text: str, text_lower: str) -> List[UnclearTermData]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ—è—Å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        unclear_terms = []
        
        for phrase in DocumentAnalyzer.UNCLEAR_PATTERNS:
            positions = DocumentAnalyzer._find_all_positions(text_lower, phrase)
            
            for pos in positions:
                context = DocumentAnalyzer._extract_context(text, pos, len(phrase))
                
                unclear_term = UnclearTermData(
                    phrase=phrase,
                    context=context,
                    position=pos,
                    explanation=f"–§—Ä–∞–∑–∞ '{phrase}' —Ç—Ä–µ–±—É–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏",
                    suggestion=DocumentAnalyzer._get_unclear_suggestion(phrase),
                    legal_clarification=DocumentAnalyzer._get_legal_clarification(phrase)
                )
                
                unclear_terms.append(unclear_term)
                break  # –û–¥–∏–Ω —Ç–µ—Ä–º–∏–Ω –Ω–∞ —Ñ—Ä–∞–∑—É
        
        return unclear_terms
    
    @staticmethod
    def _find_all_positions(text: str, keyword: str) -> List[int]:
        """–ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞"""
        positions = []
        start = 0
        
        while True:
            pos = text.find(keyword, start)
            if pos == -1:
                break
            positions.append(pos)
            start = pos + 1
        
        return positions[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –≤—Ö–æ–∂–¥–µ–Ω–∏—è
    
    @staticmethod
    def _extract_context(text: str, position: int, keyword_length: int, context_size: int = 150) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞"""
        start = max(0, position - context_size)
        end = min(len(text), position + keyword_length + context_size)
        
        context = text[start:end].strip()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–æ–µ—Ç–æ—á–∏–µ –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω
        if start > 0:
            context = "..." + context
        if end < len(text):
            context = context + "..."
        
        return context
    
    @staticmethod
    def _create_highlights(risks: List[RiskData], benefits: List[BenefitData], unclear_terms: List[UnclearTermData]) -> List[HighlightData]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥—Å–≤–µ—Ç–∫–∏"""
        highlights = []
        
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Ä–∏—Å–∫–æ–≤
        for risk in risks:
            highlight = HighlightData(
                start=risk.position,
                end=risk.position + len(risk.keyword),
                type="risk",
                text=risk.keyword,
                category=risk.type,
                severity=risk.severity
            )
            highlights.append(highlight)
        
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –≤—ã–≥–æ–¥
        for benefit in benefits:
            highlight = HighlightData(
                start=benefit.position,
                end=benefit.position + len(benefit.keyword),
                type="benefit",
                text=benefit.keyword,
                category=benefit.type
            )
            highlights.append(highlight)
        
        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –Ω–µ—è—Å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        for unclear in unclear_terms:
            highlight = HighlightData(
                start=unclear.position,
                end=unclear.position + len(unclear.phrase),
                type="unclear",
                text=unclear.phrase,
                category="–ù–µ—è—Å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞"
            )
            highlights.append(highlight)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–∑–∏—Ü–∏–∏
        highlights.sort(key=lambda x: x.start)
        
        return highlights
    
    @staticmethod
    def _calculate_rating(risks: List[RiskData], benefits: List[BenefitData], unclear_terms: List[UnclearTermData]) -> tuple:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        # –ü–æ–¥—Å—á–µ—Ç —Ä–∏—Å–∫–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º
        high_risks = len([r for r in risks if r.severity == "–≤—ã—Å–æ–∫–∏–π"])
        medium_risks = len([r for r in risks if r.severity == "—Å—Ä–µ–¥–Ω–∏–π"]) 
        low_risks = len([r for r in risks if r.severity == "–Ω–∏–∑–∫–∏–π"])
        
        # –ü–æ–¥—Å—á–µ—Ç –≤—ã–≥–æ–¥
        high_benefits = len([b for b in benefits if b.value == "–≤—ã—Å–æ–∫–∞—è"])
        medium_benefits = len([b for b in benefits if b.value == "—Å—Ä–µ–¥–Ω—è—è"])
        
        # –†–∞—Å—á–µ—Ç —á–∏—Å–ª–æ–≤–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞ (0-100)
        risk_score = (high_risks * 30 + medium_risks * 15 + low_risks * 5 + len(unclear_terms) * 3)
        risk_score = min(100, risk_score)  # –ú–∞–∫—Å–∏–º—É–º 100
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤—ã–≥–æ–¥—ã
        benefit_adjustment = (high_benefits * 10 + medium_benefits * 5)
        risk_score = max(0, risk_score - benefit_adjustment)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –æ—Ü–µ–Ω–∫–∏
        if risk_score >= 60:
            overall_rating = "—Ä–∏—Å–∫–æ–≤–∞–Ω"
        elif risk_score >= 30:
            overall_rating = "—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
        elif risk_score >= 10:
            overall_rating = "–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–µ–∑–æ–ø–∞—Å–µ–Ω"
        else:
            overall_rating = "–±–µ–∑–æ–ø–∞—Å–µ–Ω"
        
        return overall_rating, risk_score
    
    @staticmethod
    def _generate_recommendations(risks: List[RiskData], benefits: List[BenefitData], unclear_terms: List[UnclearTermData]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        if len(risks) > 0:
            recommendations.append("üîç –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏ –ø–µ—Ä–µ–¥ –ø–æ–¥–ø–∏—Å–∞–Ω–∏–µ–º")
            
        if any(r.severity == "–≤—ã—Å–æ–∫–∏–π" for r in risks):
            recommendations.append("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—ã—Å–æ–∫–∏–µ —Ä–∏—Å–∫–∏ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å —é—Ä–∏—Å—Ç–æ–º")
            
        if len(unclear_terms) > 0:
            recommendations.append("‚ùì –£—Ç–æ—á–Ω–∏—Ç–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —É –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–∞")
            
        if len(benefits) > 0:
            recommendations.append("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤—ã–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –≤ –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–∞—Ö")
            
        if len(risks) > 5:
            recommendations.append("üìã –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –¥–æ–≥–æ–≤–æ—Ä")
            
        if not recommendations:
            recommendations.append("‚ú® –î–æ–∫—É–º–µ–Ω—Ç –≤—ã–≥–ª—è–¥–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º")
            
        return recommendations
    
    @staticmethod
    def _create_summary(filename: str, risks: List[RiskData], benefits: List[BenefitData], unclear_terms: List[UnclearTermData], overall_rating: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ"""
        
        high_risks = len([r for r in risks if r.severity == "–≤—ã—Å–æ–∫–∏–π"])
        medium_risks = len([r for r in risks if r.severity == "—Å—Ä–µ–¥–Ω–∏–π"])
        
        summary = f"""üìã –ê–ù–ê–õ–ò–ó –î–û–ö–£–ú–ï–ù–¢–ê: {filename}

üéØ –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê: {overall_rating.upper()}

üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
‚Ä¢ –í—Å–µ–≥–æ —Ä–∏—Å–∫–æ–≤: {len(risks)}
  - –í—ã—Å–æ–∫–∏–µ: {high_risks}
  - –°—Ä–µ–¥–Ω–∏–µ: {medium_risks}
‚Ä¢ –í—ã–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è: {len(benefits)}
‚Ä¢ –ù–µ—è—Å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏: {len(unclear_terms)}

üí° –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:
{DocumentAnalyzer._get_key_points(risks, benefits, unclear_terms)}

‚öñÔ∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:
{DocumentAnalyzer._get_main_recommendation(overall_rating, high_risks)}

üìã –î–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ñ—Ä–∞–∑. 
–î–ª—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å —é—Ä–∏—Å—Ç–æ–º."""
        
        return summary.strip()
    
    @staticmethod
    def _get_key_points(risks: List[RiskData], benefits: List[BenefitData], unclear_terms: List[UnclearTermData]) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤"""
        points = []
        
        if risks:
            main_risk = max(risks, key=lambda r: {"–≤—ã—Å–æ–∫–∏–π": 3, "—Å—Ä–µ–¥–Ω–∏–π": 2, "–Ω–∏–∑–∫–∏–π": 1}[r.severity])
            points.append(f"‚ö†Ô∏è –û—Å–Ω–æ–≤–Ω–æ–π —Ä–∏—Å–∫: {main_risk.type}")
            
        if benefits:
            main_benefit = benefits[0]
            points.append(f"‚úÖ –ì–ª–∞–≤–Ω–∞—è –≤—ã–≥–æ–¥–∞: {main_benefit.type}")
            
        if unclear_terms:
            points.append(f"‚ùì –¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è: {unclear_terms[0].phrase}")
            
        return "\n".join(points) if points else "‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞"
    
    @staticmethod
    def _get_main_recommendation(overall_rating: str, high_risks: int) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        if overall_rating == "—Ä–∏—Å–∫–æ–≤–∞–Ω":
            return "–ù–ï —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å –±–µ–∑ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"
        elif overall_rating == "—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è":
            return "–ú–æ–∂–Ω–æ –ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ—Å–ª–µ –∏–∑—É—á–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤"
        elif high_risks > 0:
            return "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å —é—Ä–∏—Å—Ç–æ–º"
        else:
            return "–ú–æ–∂–Ω–æ –ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ –æ–∑–Ω–∞–∫–æ–º–ª–µ–Ω–∏—è —Å —É—Å–ª–æ–≤–∏—è–º–∏"
    
    @staticmethod
    def _get_risk_recommendation(risk_type: str, keyword: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ä–∏—Å–∫—É"""
        recommendations = {
            "—à—Ç—Ä–∞—Ñ–Ω—ã–µ_—Å–∞–Ω–∫—Ü–∏–∏": f"–ò–∑—É—á–∏—Ç–µ —Ä–∞–∑–º–µ—Ä—ã –∏ —É—Å–ª–æ–≤–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–Ω—ã—Ö —Å–∞–Ω–∫—Ü–∏–π –∑–∞ '{keyword}'",
            "–æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ_—É—Å–ª–æ–≤–∏—è": f"–¢—Ä–µ–±—É–π—Ç–µ –≤–∑–∞–∏–º–Ω–æ—Å—Ç–∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö '{keyword}'",
            "–≤—ã—Å–æ–∫–∞—è_–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": f"–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ '{keyword}'",
            "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ_—Å—Ä–æ–∫–∏": f"–ü–æ—Ç—Ä–µ–±—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ —Å—Ä–æ–∫–æ–≤ –≤–º–µ—Å—Ç–æ '{keyword}'",
            "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ_—Ä–∏—Å–∫–∏": f"–£—Ç–æ—á–Ω–∏—Ç–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –ø–æ '{keyword}'",
            "—Ñ–æ—Ä—Å_–º–∞–∂–æ—Ä": f"–ò–∑—É—á–∏—Ç–µ —É—Å–ª–æ–≤–∏—è —Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–∞ '{keyword}'"
        }
        return recommendations.get(risk_type, f"–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —É—Å–ª–æ–≤–∏–µ '{keyword}'")
    
    @staticmethod
    def _get_legal_reference(risk_type: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –†–ö"""
        references = {
            "—à—Ç—Ä–∞—Ñ–Ω—ã–µ_—Å–∞–Ω–∫—Ü–∏–∏": "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–ö, —Å—Ç–∞—Ç—å–∏ 350-354",
            "–æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ_—É—Å–ª–æ–≤–∏—è": "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–ö, —Å—Ç–∞—Ç—å—è 401",
            "–≤—ã—Å–æ–∫–∞—è_–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å": "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–ö, —Å—Ç–∞—Ç—å–∏ 359-364"
        }
        return references.get(risk_type)
    
    @staticmethod
    def _get_unclear_suggestion(phrase: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –Ω–µ—è—Å–Ω–æ–º—É —Ç–µ—Ä–º–∏–Ω—É"""
        suggestions = {
            "—Ä–∞–∑—É–º–Ω—ã–π —Å—Ä–æ–∫": "–£–∫–∞–∂–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞—Ç—ã –∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π",
            "–≤ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ": "–û–ø–∏—à–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É",
            "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è": "–î–∞–π—Ç–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏",
            "—Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞": "–ü—Ä–∏–≤–µ–¥–∏—Ç–µ –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫"
        }
        return suggestions.get(phrase, f"–ö–æ–Ω–∫—Ä–µ—Ç–∏–∑–∏—Ä—É–π—Ç–µ –ø–æ–Ω—è—Ç–∏–µ '{phrase}'")
    
    @staticmethod
    def _get_legal_clarification(phrase: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è"""
        clarifications = {
            "—Ä–∞–∑—É–º–Ω—ã–π —Å—Ä–æ–∫": "–ü–æ –ì–ö –†–ö —Ä–∞–∑—É–º–Ω—ã–º —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å—Ä–æ–∫, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞",
            "—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è": "–ù–∞—Ä—É—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ª–∏—à–∞—é—Ç —Å—Ç–æ—Ä–æ–Ω—É —Ç–æ–≥–æ, –Ω–∞ —á—Ç–æ –æ–Ω–∞ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–ª–∞"
        }
        return clarifications.get(phrase)

# ============================================================================
# –°–ò–°–¢–ï–ú–ê –°–û–•–†–ê–ù–ï–ù–ò–Ø –ò –£–ü–†–ê–í–õ–ï–ù–ò–Ø
# ============================================================================

class DocumentManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self):
        self.storage_dir = Path("storage")
        self.analysis_dir = self.storage_dir / "analysis"
        self.uploads_dir = self.storage_dir / "uploads"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        self.uploads_dir.mkdir(parents=True, exist_ok=True)
    
    def save_analysis(self, analysis: DocumentAnalysis) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            file_path = self.analysis_dir / f"{analysis.document_id}.json"
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(analysis.dict(), f, ensure_ascii=False, indent=2)
            
            logger.info(f"–ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
    
    def load_analysis(self, document_id: str) -> Optional[DocumentAnalysis]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            file_path = self.analysis_dir / f"{document_id}.json"
            
            if not file_path.exists():
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return DocumentAnalysis(**data)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ {document_id}: {e}")
            return None
    
    def get_saved_analyses(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤"""
        try:
            analyses = []
            
            for file_path in self.analysis_dir.glob("*.json"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    analyses.append({
                        "document_id": data.get("document_id"),
                        "filename": data.get("filename"),
                        "overall_rating": data.get("overall_rating"),
                        "risk_score": data.get("risk_score"),
                        "processed_at": data.get("processed_at"),
                        "file_size": data.get("file_size"),
                        "word_count": data.get("word_count")
                    })
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    continue
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
            analyses.sort(key=lambda x: x.get("processed_at", ""), reverse=True)
            
            return analyses
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–æ–≤: {e}")
            return []

# ============================================================================
# WEB –ü–†–ò–õ–û–ñ–ï–ù–ò–ï
# ============================================================================

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
app = FastAPI(
    title="üõ°Ô∏è SafeDocs - AI –∑–∞—â–∏—Ç–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
    description="–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ",
    version="2.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
document_manager = DocumentManager()

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Å–µ—Å—Å–∏–π (–≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ - Redis)
session_storage = {}

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
try:
    app.mount("/static", StaticFiles(directory="static"), name="static")
except Exception as e:
    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã: {e}")

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/")
async def get_main_page():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        return FileResponse('static/index.html')
    except FileNotFoundError:
        return JSONResponse(
            status_code=404,
            content={
                "message": "–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                "error": "–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª static/index.html",
                "api_docs": "/api/docs"
            }
        )

@app.post("/api/analyze", response_model=DocumentAnalysis)
async def analyze_document(file: UploadFile = File(...)):
    """–ê–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    logger.info(f"–ù–∞—á–∞—Ç –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file.filename}")
    
    try:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
        file_data = await FileProcessor.process_file(file)
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        analysis = DocumentAnalyzer.analyze(
            text=file_data['text'],
            filename=file_data['filename'],
            file_type=file_data['file_type'],
            file_size=file_data['file_size'],
            word_count=file_data['word_count']
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏–æ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        session_storage[analysis.document_id] = analysis
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {analysis.document_id}")
        
        return analysis
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        raise HTTPException(status_code=500, detail="–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞")

@app.post("/api/save")
async def save_analysis(request: dict):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    
    document_id = request.get("document_id")
    
    if not document_id:
        raise HTTPException(status_code=400, detail="ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω")
    
    if document_id not in session_storage:
        raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–µ—Å—Å–∏–∏")
    
    try:
        analysis = session_storage[document_id]
        file_path = document_manager.save_analysis(analysis)
        
        return {
            "message": "–ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω",
            "document_id": document_id,
            "file_path": file_path,
            "saved_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ {document_id}: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")

@app.get("/api/saved")
async def get_saved_documents():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    try:
        saved_docs = document_manager.get_saved_analyses()
        
        return {
            "saved_documents": saved_docs,
            "total_count": len(saved_docs)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")

@app.get("/api/documents/{document_id}")
async def get_document_analysis(document_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ ID"""
    
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Å–µ—Å—Å–∏–æ–Ω–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    if document_id in session_storage:
        return session_storage[document_id]
    
    # –ó–∞—Ç–µ–º –≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
    analysis = document_manager.load_analysis(document_id)
    
    if not analysis:
        raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    return analysis

@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_document(message: ChatMessage):
    """–ß–∞—Ç —Å AI –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É"""
    
    # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    analysis = None
    
    if message.document_id in session_storage:
        analysis = session_storage[message.document_id]
    else:
        analysis = document_manager.load_analysis(message.document_id)
    
    if not analysis:
        raise HTTPException(status_code=404, detail="–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        # –ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–≤–µ—Ç–æ–≤ (–≤ –±—É–¥—É—â–µ–º - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å AI)
        answer = DocumentAnalyzer._generate_chat_response(message.question, analysis)
        
        return ChatResponse(
            answer=answer,
            confidence=0.8,
            relevant_sections=[],
            document_id=message.document_id
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞")

@app.get("/api/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    
    return {
        "status": "healthy",
        "service": "SafeDocs",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "session_documents": len(session_storage),
        "saved_documents": len(document_manager.get_saved_analyses())
    }

@app.get("/api/stats")
async def get_statistics():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    
    try:
        saved_docs = document_manager.get_saved_analyses()
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_docs = len(saved_docs)
        ratings_count = {}
        avg_risk_score = 0
        
        for doc in saved_docs:
            rating = doc.get("overall_rating", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
            ratings_count[rating] = ratings_count.get(rating, 0) + 1
            avg_risk_score += doc.get("risk_score", 0)
        
        if total_docs > 0:
            avg_risk_score = avg_risk_score / total_docs
        
        return {
            "total_documents": total_docs,
            "session_documents": len(session_storage),
            "ratings_distribution": ratings_count,
            "average_risk_score": round(avg_risk_score, 2),
            "service_uptime": "running"
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")

# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ DocumentAnalyzer –¥–ª—è —á–∞—Ç–∞
@staticmethod
def _generate_chat_response(question: str, analysis: DocumentAnalysis) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —á–∞—Ç–µ"""
    
    question_lower = question.lower()
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    if any(word in question_lower for word in ["—Ä–∏—Å–∫", "–æ–ø–∞—Å–Ω", "—à—Ç—Ä–∞—Ñ", "–ø—Ä–æ–±–ª–µ–º"]):
        if analysis.risks:
            risks_text = "\n".join([f"‚Ä¢ {r.type}: {r.recommendation}" for r in analysis.risks[:3]])
            return f"üö® –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ä–∏—Å–∫–∏:\n\n{risks_text}\n\n–û–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {analysis.risk_score}/100"
        else:
            return "‚úÖ –°–µ—Ä—å–µ–∑–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
    
    elif any(word in question_lower for word in ["–≤—ã–≥–æ–¥", "–ø–ª—é—Å", "—Ö–æ—Ä–æ—à", "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω"]):
        if analysis.benefits:
            benefits_text = "\n".join([f"‚Ä¢ {b.type}: {b.description}" for b in analysis.benefits[:3]])
            return f"‚úÖ –í—ã–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:\n\n{benefits_text}"
        else:
            return "‚ùå –Ø–≤–Ω—ã—Ö –≤—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
    
    elif any(word in question_lower for word in ["–Ω–µ–ø–æ–Ω—è—Ç–Ω", "–Ω–µ—è—Å–Ω", "—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç", "—Ä–∞—Å—à–∏—Ñ—Ä"]):
        if analysis.unclear_terms:
            unclear_text = "\n".join([f"‚Ä¢ {u.phrase}: {u.suggestion}" for u in analysis.unclear_terms[:3]])
            return f"‚ùì –ù–µ—è—Å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:\n\n{unclear_text}"
        else:
            return "‚úÖ –í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–Ω—è—Ç–Ω—ã."
    
    elif any(word in question_lower for word in ["–ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å", "—Å–æ–≥–ª–∞—Å–∏—Ç—å—Å—è", "—Å—Ç–æ–∏—Ç –ª–∏", "—Ä–µ–∫–æ–º–µ–Ω–¥"]):
        recommendations_text = "\n".join([f"‚Ä¢ {rec}" for rec in analysis.recommendations])
        return f"""
ü§î –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É "{analysis.filename}":

üìä –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {analysis.overall_rating}
üìà –£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {analysis.risk_score}/100

üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
{recommendations_text}

‚öñÔ∏è –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ—Å—Ç–∞–µ—Ç—Å—è –∑–∞ –≤–∞–º–∏. –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Å–æ–º–Ω–µ–Ω–∏–π —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å —é—Ä–∏—Å—Ç–æ–º.
        """.strip()
    
    elif any(word in question_lower for word in ["—Ä–µ–∑—é–º–µ", "–∏—Ç–æ–≥", "–∫—Ä–∞—Ç–∫–æ", "—Å—É—Ç—å"]):
        return analysis.summary
    
    else:
        return f"""
üí¨ –°–ø–∞—Å–∏–±–æ –∑–∞ –≤–æ–ø—Ä–æ—Å! 

–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ "{analysis.filename}":

üîç –î–æ—Å—Ç—É–ø–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —Ä–∏—Å–∫–æ–≤: {len(analysis.risks)}
‚Ä¢ –í—ã–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π: {len(analysis.benefits)}
‚Ä¢ –ù–µ—è—Å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫: {len(analysis.unclear_terms)}
‚Ä¢ –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {analysis.overall_rating}

‚ùì –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–ø—Ä–æ—Å–∏—Ç—å:
‚Ä¢ "–ö–∞–∫–∏–µ –µ—Å—Ç—å —Ä–∏—Å–∫–∏?"
‚Ä¢ "–ï—Å—Ç—å –ª–∏ –≤—ã–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è?"
‚Ä¢ "–°—Ç–æ–∏—Ç –ª–∏ –ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å?"
‚Ä¢ "–ß—Ç–æ –æ–∑–Ω–∞—á–∞—é—Ç –Ω–µ—è—Å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã?"

ü§ñ –°–∏—Å—Ç–µ–º–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
        """

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –≤ –∫–ª–∞—Å—Å
DocumentAnalyzer._generate_chat_response = _generate_chat_response

# ============================================================================
# –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("üõ°Ô∏è  SafeDocs v2.0 - AI –∑–∞—â–∏—Ç–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("=" * 60)
    print("üåê –í–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: http://localhost:8000")
    print("üìö API –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/api/docs")
    print("üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API: http://localhost:8000/api/redoc")
    print("üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: ./storage/")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–ø–æ–∫
    Path("static").mkdir(exist_ok=True)
    Path("storage").mkdir(exist_ok=True)
    Path("storage/analysis").mkdir(exist_ok=True)
    Path("storage/uploads").mkdir(exist_ok=True)
    
    
    
    
    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    uvicorn.run(
        "main:app",
        host="127.0.0.1",
        port=8000,
        reload=True,
        log_level="info"
    )
    
    
  